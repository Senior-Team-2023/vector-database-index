{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj9-3U--Krvc"
      },
      "source": [
        "# This Notebook for Running the ADB Project Phase 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV2Nc_f8Mbqh"
      },
      "source": [
        "**This notebook is divided into two main parts, each focusing on different database sizes:**\n",
        "\n",
        "- **Part 1: Database Size 10K**\n",
        "\n",
        "  - Initiate a new database and insert vectors into it.\n",
        "  - Retrieve vectors from the database.\n",
        "  - Ensure that the insertion time for this database does not exceed 5 minutes.\n",
        "  - Allow flexible RAM usage during insertion but ensure it stays within Google Colab limits.\n",
        "  - Evaluate retrieval time and accuracy.\n",
        "  - Ensure that the peak RAM usage for retrieval does not exceed 5 MB.\n",
        "\n",
        "- **Part 2: Database Sizes 100K and More**\n",
        "  - Generate database vectors using a random seed (refer to the provided code).\n",
        "  - You have generate the database and its index before the submission.\n",
        "  - Implement a VecDB class that loads the pre-generated database, including the index, and retrieves vectors, to load the generated database.\n",
        "  - Evaluate retrieval time and accuracy for different database sizes.\n",
        "  - The Peak RAM usage for the retrieval should not exceed\n",
        "    - For 100 K --> 10 MB\n",
        "    - For 1 M --> 25 MB\n",
        "    - For 5 M --> 75 MB\n",
        "    - For 10 M --> 150 MB\n",
        "    - For 15 M --> 225 MB\n",
        "    - For 20 M --> 300 MB\n",
        "\n",
        "**This notebook is structured into two parts:**\n",
        "\n",
        "- **Part 1 - Modifiable Cells:**\n",
        "  This section contains cells that teams are allowed to modify. The modification are only variables and to be submitted during the project's final phase. They are\n",
        "\n",
        "  - GitHub repository link (including PAT token).\n",
        "  - Database (DB) variables, providing the path to the directory or file for loading existing databases and indexes (refer to provided code to see how).\n",
        "\n",
        "- **Part 2 - Non-Modifiable Cells:** This section must not be modified by any team. It includes essential setup and evaluation code. Ensure that the notebook runs smoothly by providing the required inputs in Part 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4EV_xB6Kw17"
      },
      "source": [
        "## Part 1 - Modifiable Cells\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AODP-iztLtBV"
      },
      "source": [
        "Of course each team will provide different github repo link\n",
        "Should include PAT token to enable me to download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCR6Z8ABxE3w",
        "outputId": "70c67a27-5c28-4086-8f39-b33845524a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'sematic_search_DB'...\n",
            "warning: redirecting to https://github.com/abdokaseb/sematic_search_DB.git/\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 19 (delta 7), reused 13 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (19/19), 6.30 KiB | 6.30 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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:@github.com/abdokaseb/sematic_search_DB.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsUXWYom6xRv"
      },
      "source": [
        "Teams are required to provide unique paths for the generated databases of sizes 1M, 5M, 10M, 15M, and 20M. Follow these steps to submit the databases:\n",
        "\n",
        "- Once you have the database and index ready, zip the necessary folders/files.\n",
        "- Upload the zip file to Google Drive.\n",
        "- Ensure the file is shareable with \"anyone with the link.\"\n",
        "- Obtain the zip file link (e.g., https://drive.google.com/file/d/1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah/view?usp=drive_link).\n",
        "- Extract the zip file ID (e.g., 1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah).\n",
        "- Place the ID in the designated variable (to be submitted during the project final phase).\n",
        "- The code will automatically download the zip file and unzip it inside this directory.\n",
        "- Provide the local PATH for each database to be passed to the initializer for automatic loading of the database and index (to be submitted during the project final phase).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kK46_ZVe5L3u"
      },
      "outputs": [],
      "source": [
        "TEAM_NUMBER = 2\n",
        "GDRIVE_ID_DB_100K = \"14Gp_C3LxLuYIyF-zL5q-xFXKQ8KOFtZp\"\n",
        "GDRIVE_ID_DB_1M = \"1XbEP6sU0k0UbuPcQLkHJP7cdPtebpsbD\"\n",
        "GDRIVE_ID_DB_5M = \"1DX0tw9YDlvRthjMq3LQ6_aUyp3BvTC1r\"\n",
        "GDRIVE_ID_DB_10M = \"1Jho9rair77eWdA5-iI_qDT2spJNesuDe\"\n",
        "GDRIVE_ID_DB_15M = \"1sPIgNxIuNDUUBnTvAuPMLryd1mqmgwV3\"\n",
        "GDRIVE_ID_DB_20M = \"1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah\"\n",
        "PATH_DB_100K = \"saved_db_100k.csv\"\n",
        "PATH_DB_1M = \"saved_db_1m.csv\"\n",
        "PATH_DB_5M = \"saved_db_5m.csv\"\n",
        "PATH_DB_10M = \"saved_db_10m.csv\"\n",
        "PATH_DB_15M = \"saved_db_15m.csv\"\n",
        "PATH_DB_20M = \"saved_db_20m.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGLg01fsujm"
      },
      "source": [
        "These two varaible I'll change while running in on the discussion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G44iH6jnObEj"
      },
      "outputs": [],
      "source": [
        "QUERY_SEED_NUMBER = 10\n",
        "DB_SEED_NUMBER = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWaZ-ByWOIcK"
      },
      "source": [
        "This means that the project submission will include these\n",
        "\n",
        "- TEAM_NUMBER\n",
        "- Github clone link\n",
        "- GDRIVE_ID_DB_100K\n",
        "- GDRIVE_ID_DB_1M\n",
        "- GDRIVE_ID_DB_5M\n",
        "- GDRIVE_ID_DB_10M\n",
        "- GDRIVE_ID_DB_15M\n",
        "- GDRIVE_ID_DB_20M\n",
        "- PATH_DB_100K\n",
        "- PATH_DB_1M\n",
        "- PATH_DB_5M\n",
        "- PATH_DB_10M\n",
        "- PATH_DB_15M\n",
        "- PATH_DB_20M <br>\n",
        "- And for sure the project document that describes what you did\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzFTOecwu8wj"
      },
      "source": [
        "## Part 2: No edits from here\n",
        "\n",
        "#### You can't edit this part, and neither me.\n",
        "\n",
        "#### Note: Maybe I can edit if there is a major bug\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqujj7tYTA1l",
        "outputId": "66366264-6039-4bef-f56d-9426fd1ea11a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/sematic_search_DB\n"
          ]
        }
      ],
      "source": [
        "%cd sematic_search_DB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJmXzFdisD7P"
      },
      "source": [
        "This cell to run any additional requirement that your code need <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaPjq2hMqd20",
        "outputId": "f59fab92-3790-4188-960a-1ad34b4772fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.24.3 in c:\\users\\peter\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: scikit_learn==1.3.0 in c:\\users\\peter\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: scipy==1.11.3 in c:\\users\\peter\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 7)) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\peter\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit_learn==1.3.0->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\peter\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit_learn==1.3.0->-r requirements.txt (line 5)) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install memory-profiler >> log.txt\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG0DALR498__"
      },
      "source": [
        "This cell to download the zip files and unzip them here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSv2z0PVp6HA",
        "outputId": "29d46d78-dca9-4764-a346-78f2574d075a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14Gp_C3LxLuYIyF-zL5q-xFXKQ8KOFtZp\n",
            "To: /content/sematic_search_DB/saved_db_100k.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 57.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XbEP6sU0k0UbuPcQLkHJP7cdPtebpsbD\n",
            "To: /content/sematic_search_DB/saved_db_1m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 58.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DX0tw9YDlvRthjMq3LQ6_aUyp3BvTC1r\n",
            "To: /content/sematic_search_DB/saved_db_5m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 80.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Jho9rair77eWdA5-iI_qDT2spJNesuDe\n",
            "To: /content/sematic_search_DB/saved_db_10m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 54.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sPIgNxIuNDUUBnTvAuPMLryd1mqmgwV3\n",
            "To: /content/sematic_search_DB/saved_db_15m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 66.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah\n",
            "To: /content/sematic_search_DB/saved_db_20m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 137MB/s]\n",
            "Archive:  saved_db_100k.zip\n",
            "  inflating: saved_db_100k.csv       \n",
            "Archive:  saved_db_1m.zip\n",
            "  inflating: saved_db_1m.csv         \n",
            "Archive:  saved_db_5m.zip\n",
            "  inflating: saved_db_5m.csv         \n",
            "Archive:  saved_db_10m.zip\n",
            "  inflating: saved_db_10m.csv        \n",
            "Archive:  saved_db_15m.zip\n",
            "  inflating: saved_db_15m.csv        \n",
            "Archive:  saved_db_20m.zip\n",
            "  inflating: saved_db_20m.csv        \n"
          ]
        }
      ],
      "source": [
        "!gdown $GDRIVE_ID_DB_100K -O saved_db_100k.zip\n",
        "!gdown $GDRIVE_ID_DB_1M -O saved_db_1m.zip\n",
        "!gdown $GDRIVE_ID_DB_5M -O saved_db_5m.zip\n",
        "!gdown $GDRIVE_ID_DB_10M -O saved_db_10m.zip\n",
        "!gdown $GDRIVE_ID_DB_15M -O saved_db_15m.zip\n",
        "!gdown $GDRIVE_ID_DB_20M -O saved_db_20m.zip\n",
        "!unzip saved_db_100k.zip\n",
        "!unzip saved_db_1m.zip\n",
        "!unzip saved_db_5m.zip\n",
        "!unzip saved_db_10m.zip\n",
        "!unzip saved_db_15m.zip\n",
        "!unzip saved_db_20m.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShuPR-gGlX3f"
      },
      "source": [
        "These are the functions for running and reporting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sg2vfYgeyavn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from ivf import VecDB\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from memory_profiler import memory_usage\n",
        "import gc\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    run_time: float\n",
        "    top_k: int\n",
        "    db_ids: List[int]\n",
        "    actual_ids: List[int]\n",
        "\n",
        "\n",
        "results = []\n",
        "to_print_arr = []\n",
        "\n",
        "\n",
        "def run_queries(db, query, top_k, actual_ids, num_runs):\n",
        "    global results\n",
        "    results = []\n",
        "    for _ in range(num_runs):\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrive(query, top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids))\n",
        "    return results\n",
        "\n",
        "\n",
        "def memory_usage_run_queries(args):\n",
        "    global results\n",
        "    # This part is added to calcauate the RAM usage\n",
        "    mem_before = max(memory_usage())\n",
        "    mem = memory_usage(proc=(run_queries, args, {}), interval=1e-3)\n",
        "    return results, max(mem) - mem_before\n",
        "\n",
        "\n",
        "def evaluate_result(results: List[Result]):\n",
        "    # scores are negative. So getting 0 is the best score.\n",
        "    scores = []\n",
        "    run_time = []\n",
        "    for res in results:\n",
        "        run_time.append(res.run_time)\n",
        "        # case for retireving number not equal to top_k, socre will be the lowest\n",
        "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
        "            scores.append(-1 * len(res.actual_ids) * res.top_k)\n",
        "            continue\n",
        "        score = 0\n",
        "        for id in res.db_ids:\n",
        "            try:\n",
        "                ind = res.actual_ids.index(id)\n",
        "                if ind > res.top_k * 3:\n",
        "                    score -= ind\n",
        "            except:\n",
        "                score -= len(res.actual_ids)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
        "\n",
        "\n",
        "def get_actual_ids_first_k(actual_sorted_ids, k):\n",
        "    return [id for id in actual_sorted_ids if id < k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3bQQzzWlce4"
      },
      "source": [
        "This to generate 10K, 100K databases and the query using the seed numbers that will be changed at submissions day\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "82Mb008w5YB7"
      },
      "outputs": [],
      "source": [
        "# rng = np.random.default_rng(DB_SEED_NUMBER)\n",
        "rng = np.random.default_rng(50)\n",
        "vectors = rng.random((2*10**6, 70), dtype=np.float32)\n",
        "\n",
        "rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
        "query = rng.random((1, 70), dtype=np.float32)\n",
        "\n",
        "actual_sorted_ids_10k = (\n",
        "    np.argsort(\n",
        "        vectors.dot(query.T).T\n",
        "        / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query)),\n",
        "        axis=1,\n",
        "    )\n",
        "    .squeeze()\n",
        "    .tolist()[::-1]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiofTQ56l1wz"
      },
      "source": [
        "Open new DB add 10K then retrieve and evaluate. Then add another 90K (total 100K) then retrieve and evaluate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "broiY85IyDZ6",
        "outputId": "07e1ccfd-8e83-4b06-ab63-c50e13b68404"
      },
      "outputs": [],
      "source": [
        "records_dict = [{\"id\": i, \"embed\": list(row)} for i, row in enumerate(vectors)]\n",
        "n = len(records_dict)\n",
        "db = VecDB(\n",
        "    file_path=PATH_DB_100K\n",
        "    if n == 10**5\n",
        "    else PATH_DB_1M\n",
        "    if n == 10**6\n",
        "    else PATH_DB_5M\n",
        "    if n == 5 * 10**6\n",
        "    else PATH_DB_10M\n",
        "    if n == 10 * 10**6\n",
        "    else PATH_DB_15M\n",
        "    if n == 15 * 10**6\n",
        "    else PATH_DB_20M\n",
        "    if n == 20 * 10**6\n",
        "    else \"saved_db.csv\"\n",
        ")\n",
        "\n",
        "batch_size = min(10**6, int(n * 0.1)) if n >= 10**6 else n\n",
        "\n",
        "print(\"batch_size:\", batch_size)\n",
        "\n",
        "flag = False\n",
        "\n",
        "for i in range(0, n // batch_size):\n",
        "\n",
        "    flag = i == (n // batch_size - 1)\n",
        "\n",
        "    # print(records_dict[0])\n",
        "\n",
        "    db.insert_records(records_dict[0:batch_size], flag)\n",
        "\n",
        "    del records_dict[0:batch_size]\n",
        "\n",
        "# db.insert_records(records_dict)\n",
        "\n",
        "res = run_queries(\n",
        "    db, query, 5, actual_sorted_ids_10k, 1\n",
        ")  # one run to make everything fresh and loaded\n",
        "\n",
        "res, mem = memory_usage_run_queries(\n",
        "    (db, query, 5, actual_sorted_ids_10k, 5)\n",
        ")  # actual runs to compute time, and memory\n",
        "\n",
        "eval = evaluate_result(res)\n",
        "\n",
        "to_print = f\"10K\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "\n",
        "to_print_arr.append(to_print)\n",
        "\n",
        "print(to_print)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZy7uYKeFQ-K"
      },
      "source": [
        "Remove exsiting varaibles to empty some RAM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MreLqUWLn2uX",
        "outputId": "a143ca3b-787f-43cd-a3a3-f8de460e0cc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del vectors\n",
        "del query\n",
        "del actual_sorted_ids_10k\n",
        "del records_dict\n",
        "del db\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrOlipAOmy9K"
      },
      "source": [
        "This code to generate 20M database. The seed (50) will not be changed. Create the same DB and prepare it's files indexes and every related file. <br>\n",
        "Note at the submission I'll not run the insert records. <br>\n",
        "The query istelf will be changed at submissions day but not the DB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c83ybYSKK85G"
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(50)\n",
        "vectors = rng.random((10**7 * 2, 70), dtype=np.float32)\n",
        "\n",
        "rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
        "query = rng.random((1, 70), dtype=np.float32)\n",
        "\n",
        "actual_sorted_ids_20m = (\n",
        "    np.argsort(\n",
        "        vectors.dot(query.T).T\n",
        "        / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query)),\n",
        "        axis=1,\n",
        "    )\n",
        "    .squeeze()\n",
        "    .tolist()[::-1]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzkAn4AF19Jl",
        "outputId": "0c8c9879-87a0-45c1-d6ef-e4290801dc53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100K\tscore\t-307208.0\ttime\t7.82\tRAM\t1.75 MB\n",
            "1M\tscore\t-3062013.0\ttime\t8.00\tRAM\t0.23 MB\n",
            "5M\tscore\t-15309954.0\ttime\t8.10\tRAM\t0.00 MB\n",
            "10M\tscore\t-30614896.0\ttime\t7.44\tRAM\t0.00 MB\n",
            "15M\tscore\t-45922276.0\ttime\t7.44\tRAM\t0.00 MB\n"
          ]
        }
      ],
      "source": [
        "db = VecDB(file_path=PATH_DB_100K, new_db=False)\n",
        "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**5)\n",
        "res = run_queries(\n",
        "    db, query, 5, actual_ids, 1\n",
        ")  # one run to make everything fresh and loaded\n",
        "res, mem = memory_usage_run_queries(\n",
        "    (db, query, 5, actual_ids, 3)\n",
        ")  # actual runs to compute time, and memory\n",
        "eval = evaluate_result(res)\n",
        "to_print = f\"100K\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "to_print_arr.append(to_print)\n",
        "print(to_print)\n",
        "\n",
        "\n",
        "db = VecDB(file_path=PATH_DB_1M, new_db=False)\n",
        "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6)\n",
        "res = run_queries(\n",
        "    db, query, 5, actual_ids, 1\n",
        ")  # one run to make everything fresh and loaded\n",
        "res, mem = memory_usage_run_queries(\n",
        "    (db, query, 5, actual_ids, 3)\n",
        ")  # actual runs to compute time, and memory\n",
        "eval = evaluate_result(res)\n",
        "to_print = f\"1M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "to_print_arr.append(to_print)\n",
        "print(to_print)\n",
        "\n",
        "\n",
        "db = VecDB(file_path=PATH_DB_5M, new_db=False)\n",
        "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6 * 5)\n",
        "res = run_queries(\n",
        "    db, query, 5, actual_ids, 1\n",
        ")  # one run to make everything fresh and loaded\n",
        "res, mem = memory_usage_run_queries(\n",
        "    (db, query, 5, actual_ids, 3)\n",
        ")  # actual runs to compute time, and memory\n",
        "eval = evaluate_result(res)\n",
        "to_print = f\"5M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "to_print_arr.append(to_print)\n",
        "print(to_print)\n",
        "\n",
        "\n",
        "db = VecDB(file_path=PATH_DB_10M, new_db=False)\n",
        "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6 * 10)\n",
        "res = run_queries(\n",
        "    db, query, 5, actual_ids, 1\n",
        ")  # one run to make everything fresh and loaded\n",
        "res, mem = memory_usage_run_queries(\n",
        "    (db, query, 5, actual_ids, 3)\n",
        ")  # actual runs to compute time, and memory\n",
        "eval = evaluate_result(res)\n",
        "to_print = f\"10M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "to_print_arr.append(to_print)\n",
        "print(to_print)\n",
        "\n",
        "\n",
        "db = VecDB(file_path=PATH_DB_15M, new_db=False)\n",
        "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6 * 15)\n",
        "res = run_queries(\n",
        "    db, query, 5, actual_ids, 1\n",
        ")  # one run to make everything fresh and loaded\n",
        "res, mem = memory_usage_run_queries(\n",
        "    (db, query, 5, actual_ids, 3)\n",
        ")  # actual runs to compute time, and memory\n",
        "eval = evaluate_result(res)\n",
        "to_print = f\"15M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "to_print_arr.append(to_print)\n",
        "print(to_print)\n",
        "\n",
        "\n",
        "db = VecDB(file_path=PATH_DB_20M, new_db=False)\n",
        "actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, 10**6 * 20)\n",
        "res = run_queries(\n",
        "    db, query, 5, actual_ids, 1\n",
        ")  # one run to make everything fresh and loaded\n",
        "res, mem = memory_usage_run_queries(\n",
        "    (db, query, 5, actual_ids, 3)\n",
        ")  # actual runs to compute time, and memory\n",
        "eval = evaluate_result(res)\n",
        "to_print = f\"20M\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "to_print_arr.append(to_print)\n",
        "print(to_print)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt1_7ihfB37Z"
      },
      "outputs": [],
      "source": [
        "print(\"Team Number\", TEAM_NUMBER)\n",
        "print(\"\\n\".join(to_print_arr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YneWf9F0oS2J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a01AEyyiXp56"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
